import logging
from llm_client import get_llm_client
import io
import logging
from contextlib import redirect_stdout
from datetime import datetime
import os


def export_evaluations_to_html(
        query: str,
        retrieved_context: dict[str, any] = None,
        generated_answer: str = None,
        rag_answer: str = None,
        llm_only_answer: str = None,
        llm_client: any = None,
        ground_truth: str = None,
        retrieval_metrics: list[str] = ["relevance", "coverage", "diversity"],
        answer_metrics: list[str] = ["factuality", "completeness", "conciseness", "coherence"],
        output_file: str = None
) -> str:
    """
    Captures the output of all evaluation functions and formats the results into a nice HTML file.

    Args:
        query: The original user query
        retrieved_context: The context dictionary returned by retrieve_context()
        generated_answer: The answer generated by the RAG system
        rag_answer: The answer generated by the RAG system (if comparing with LLM-only)
        llm_only_answer: The answer generated by using only the LLM
        llm_client: An LLM client for evaluation scoring
        ground_truth: Optional ground truth answer for comparison
        retrieval_metrics: List of metrics to evaluate retrieval
        answer_metrics: List of metrics to evaluate the generated answer
        output_file: Optional file path to save the HTML output. If None, a default filename will be used.

    Returns:
        The path to the generated HTML file
    """
    # Import the evaluation functions directly instead of using relative imports
    import evaluate_retrieval

    # Create a default output filename if none provided
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"evaluation_results_{timestamp}.html"

    # Prepare to capture stdout
    retrieval_output = ""
    answer_output = ""
    comparison_output = ""

    # Capture retrieval evaluation output if context is provided
    if retrieved_context and llm_client:
        buffer = io.StringIO()
        with redirect_stdout(buffer):
            evaluate_retrieval.evaluate_retrieval(
                retrieved_context=retrieved_context,
                query=query,
                llm_client=llm_client,
                metrics=retrieval_metrics
            )
        retrieval_output = buffer.getvalue()

    # Capture answer evaluation output if an answer is provided
    if generated_answer and llm_client:
        buffer = io.StringIO()
        with redirect_stdout(buffer):
            evaluate_retrieval.evaluate_generated_answer(
                query=query,
                retrieved_context=retrieved_context or {},
                generated_answer=generated_answer,
                llm_client=llm_client,
                ground_truth=ground_truth,
                metrics=answer_metrics
            )
        answer_output = buffer.getvalue()

    # Capture RAG vs LLM comparison output if both answers are provided
    if rag_answer and llm_only_answer and llm_client:
        buffer = io.StringIO()
        with redirect_stdout(buffer):
            evaluate_retrieval.evaluate_rag_vs_llm(
                query=query,
                rag_answer=rag_answer,
                llm_only_answer=llm_only_answer,
                llm_client=llm_client
            )
        comparison_output = buffer.getvalue()

    # Function to convert plain text output to HTML sections
    def text_to_html_section(text, section_title):
        # Skip empty sections
        if not text.strip():
            return ""

        # Process metrics section specially
        lines = text.split('\n')
        html_lines = []

        in_metrics = False
        metrics_html = ""

        for line in lines:
            # Detect metrics section
            if line.strip() == "Metrics:":
                in_metrics = True
                metrics_html = "<h3>Metrics</h3>\n<div class='metrics'>\n"
                continue

            # Handle metrics lines
            if in_metrics and line.strip().startswith("- "):
                # Parse the metric line, e.g. "- Factuality: 8/10"
                parts = line.strip()[2:].split(":")
                if len(parts) == 2:
                    metric_name = parts[0].strip()
                    score_part = parts[1].strip()

                    # Handle score format "8/10"
                    if "/" in score_part:
                        score_value = score_part.split("/")[0].strip()
                        try:
                            score = int(score_value)
                            # Create progress bar for the score
                            bar_color = "green" if score >= 7 else "orange" if score >= 4 else "red"
                            metrics_html += f"""
                            <div class='metric'>
                                <span class='metric-name'>{metric_name}:</span>
                                <span class='metric-score'>{score_part}</span>
                                <div class='metric-bar'>
                                    <div class='metric-progress' style='width:{score * 10}%; background-color:{bar_color};'></div>
                                </div>
                            </div>
                            """
                            continue
                        except ValueError:
                            pass

                # Fallback for metrics lines that couldn't be parsed
                metrics_html += f"<div class='metric'>{line.strip()[2:]}</div>\n"
                continue

            # End of metrics section
            if in_metrics and (
                    not line.strip() or line.strip().startswith("Analysis:") or line.strip().startswith("Overall")):
                in_metrics = False
                metrics_html += "</div>\n"
                html_lines.append(metrics_html)

            # Handle section headers
            if line.strip().startswith("====="):
                continue  # Skip section separator lines
            elif line.strip().startswith("Analysis:"):
                # Extract analysis text
                analysis_text = line.strip()[9:].strip()
                html_lines.append(f"<h3>Analysis</h3>\n<p class='analysis'>{analysis_text}</p>")
                continue
            elif line.strip().startswith("Improvement Suggestions:"):
                # Extract suggestions text
                suggestions_text = line.strip()[24:].strip()
                suggestions_html = "<h3>Improvement Suggestions</h3>\n<ul class='suggestions'>\n"

                # Try to split into bullet points if possible
                suggestions_items = suggestions_text.split(". ")
                if len(suggestions_items) > 1:
                    for item in suggestions_items:
                        if item.strip():
                            suggestions_html += f"<li>{item.strip()}</li>\n"
                else:
                    suggestions_html += f"<li>{suggestions_text}</li>\n"

                suggestions_html += "</ul>\n"
                html_lines.append(suggestions_html)
                continue
            elif line.strip().startswith("When RAG provides advantages:"):
                # Extract RAG advantages text
                advantages_text = line.strip()[27:].strip()
                html_lines.append(f"<h3>When RAG Provides Advantages</h3>\n<p class='advantages'>{advantages_text}</p>")
                continue
            elif line.strip().startswith("When LLM-only approach is sufficient:"):
                # Extract LLM-only sufficiency text
                sufficiency_text = line.strip()[39:].strip()
                html_lines.append(
                    f"<h3>When LLM-Only Approach Is Sufficient</h3>\n<p class='sufficiency'>{sufficiency_text}</p>")
                continue
            elif line.strip().startswith("Overall Scores:"):
                html_lines.append("<h3>Overall Scores</h3>\n<div class='overall-scores'>")
                continue
            elif line.strip().startswith("- Overall Winner:"):
                winner = line.strip()[18:].strip()
                html_lines.append(f"<p class='winner'><strong>Winner:</strong> {winner}</p>\n</div>")
                continue
            elif line.strip().startswith("- RAG System:") or line.strip().startswith("- LLM-Only System:"):
                html_lines.append(f"<p>{line.strip()[2:]}</p>")
                continue

            # Normal line
            if line.strip() and not in_metrics:
                # Handle basic info lines at the beginning
                if ":" in line and not line.startswith(" "):
                    parts = line.split(":", 1)
                    html_lines.append(f"<p><strong>{parts[0]}:</strong>{parts[1]}</p>")
                else:
                    html_lines.append(f"<p>{line}</p>")

        # Create the complete section
        section_html = f"<section class='evaluation-section'>\n"
        section_html += f"<h2>{section_title}</h2>\n"
        section_html += "\n".join(html_lines)
        section_html += "\n</section>\n"

        return section_html

    # Create the HTML content
    html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAG Evaluation Results</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }}
        header {{
            background-color: #3498db;
            color: white;
            padding: 20px;
            border-radius: 5px;
            margin-bottom: 30px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        h1 {{
            margin: 0;
            font-size: 28px;
        }}
        .evaluation-meta {{
            background-color: #eaf2f8;
            padding: 15px;
            border-radius: 5px;
            margin-bottom: 20px;
            border-left: 5px solid #3498db;
        }}
        .evaluation-section {{
            background-color: white;
            padding: 20px;
            border-radius: 5px;
            margin-bottom: 30px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        h2 {{
            color: #2c3e50;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 10px;
            font-size: 22px;
        }}
        h3 {{
            color: #3498db;
            margin-top: 20px;
            font-size: 18px;
        }}
        .metrics {{
            margin: 15px 0;
        }}
        .metric {{
            margin-bottom: 10px;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
        }}
        .metric-name {{
            font-weight: bold;
            width: 150px;
        }}
        .metric-score {{
            width: 50px;
            text-align: center;
        }}
        .metric-bar {{
            flex-grow: 1;
            height: 15px;
            background-color: #ecf0f1;
            border-radius: 10px;
            margin-left: 10px;
            overflow: hidden;
        }}
        .metric-progress {{
            height: 100%;
            border-radius: 10px;
        }}
        .analysis, .advantages, .sufficiency {{
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            border-left: 3px solid #3498db;
        }}
        .suggestions {{
            padding-left: 30px;
        }}
        .suggestions li {{
            margin-bottom: 5px;
        }}
        .overall-scores p {{
            margin: 5px 0;
        }}
        .winner {{
            font-size: 18px;
            color: #2c3e50;
        }}
        .answer-container {{
            background-color: #f1f9f1;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            border-left: 3px solid #27ae60;
            white-space: pre-wrap;
            font-family: monospace;
            max-height: 300px;
            overflow-y: auto;
        }}
        footer {{
            text-align: center;
            margin-top: 30px;
            color: #7f8c8d;
            font-size: 14px;
        }}
        @media print {{
            body {{
                background-color: white;
            }}
            .evaluation-section {{
                break-inside: avoid;
                box-shadow: none;
                border: 1px solid #e0e0e0;
            }}
            .answer-container {{
                max-height: none;
            }}
        }}
    </style>
</head>
<body>
    <header>
        <h1>RAG Evaluation Results</h1>
    </header>

    <div class="evaluation-meta">
        <p><strong>Query:</strong> {query}</p>
        <p><strong>Date:</strong> {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
    </div>
    """

    # Add original answers if available
    if generated_answer or rag_answer:
        html_content += "<section class='evaluation-section'>\n"
        html_content += "<h2>Generated Answers</h2>\n"

        if generated_answer and not rag_answer:
            html_content += "<h3>RAG Generated Answer</h3>\n"
            html_content += f"<div class='answer-container'>{generated_answer}</div>\n"

        if rag_answer:
            html_content += "<h3>RAG Generated Answer</h3>\n"
            html_content += f"<div class='answer-container'>{rag_answer}</div>\n"

        if llm_only_answer:
            html_content += "<h3>LLM-Only Generated Answer</h3>\n"
            html_content += f"<div class='answer-container'>{llm_only_answer}</div>\n"

        if ground_truth:
            html_content += "<h3>Ground Truth Answer</h3>\n"
            html_content += f"<div class='answer-container'>{ground_truth}</div>\n"

        html_content += "</section>\n"

    # Add evaluation sections
    if retrieval_output:
        html_content += text_to_html_section(retrieval_output, "Retrieval Evaluation")

    if answer_output:
        html_content += text_to_html_section(answer_output, "Answer Evaluation")

    if comparison_output:
        html_content += text_to_html_section(comparison_output, "RAG vs LLM-Only Comparison")

    # Add footer and close HTML tags
    html_content += """
    <footer>
        <p>Generated by the RAG Evaluation System</p>
    </footer>
</body>
</html>
    """

    # Write the HTML file
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(html_content)

        print(f"Evaluation results exported to: {os.path.abspath(output_file)}")
        return os.path.abspath(output_file)
    except Exception as e:
        logging.error(f"Failed to write HTML file: {e}")
        print(f"Failed to write HTML file: {e}")
        return None
def evaluate_retrieval(
        retrieved_context: dict[str, any],
        query: str,
        llm_client: any,
        metrics: list[str] = ["relevance", "coverage", "diversity"]
) -> None:
    """
    Evaluates the quality of retrieved context against the query and prints the results.

    Args:
        retrieved_context: The context dictionary returned by retrieve_context()
        query: The original user query
        llm_client: An LLM client for evaluation scoring
        metrics: List of metrics to evaluate (relevance, coverage, diversity)
    """
    logging.info("Evaluating retrieval quality...")
    print("\n===== Retrieval Evaluation =====")

    # Print basic stats
    print(f"Query: {query}")
    print(f"Total entities retrieved: {len(retrieved_context.get('top_entities', []))}")
    print(f"Total code chunks retrieved: {len(retrieved_context.get('relevant_chunks', []))}")
    print(f"Total graph nodes: {len(retrieved_context.get('related_graph_nodes', []))}")
    print(f"Total graph relationships: {len(retrieved_context.get('related_graph_relationships', []))}")

    # Skip detailed evaluation if no context was retrieved
    if (not retrieved_context.get("top_entities") and
            not retrieved_context.get("relevant_chunks") and
            not retrieved_context.get("related_graph_nodes")):
        print("Analysis: No context retrieved for evaluation.")
        for metric in metrics:
            print(f"{metric.capitalize()}: 0.0/10")
        return

    # Extract sample content for evaluation
    context_samples = []

    # Add top entities
    for entity in retrieved_context.get("top_entities", [])[:5]:
        meta = entity.get('metadata', {})
        desc = entity.get('description', '')
        entity_type = meta.get('entity_type', 'Unknown')
        name = meta.get('name', 'Unknown')
        context_samples.append(f"{entity_type} '{name}': {desc[:200]}")

    # Add top chunks
    for chunk in retrieved_context.get("relevant_chunks", [])[:5]:
        meta = chunk.get('metadata', {})
        doc = chunk.get('document', '')
        source = meta.get('source_file', 'Unknown')
        context_samples.append(f"Code from {source}: {doc[:200]}")

    # Add node info
    for node in retrieved_context.get("related_graph_nodes", [])[:5]:
        props = node.get('properties', {})
        desc = props.get('description') or props.get('docstring') or ''
        name = props.get('name', 'Unknown')
        context_samples.append(f"Node '{name}': {desc[:200]}")

    # Create evaluation prompt
    evaluation_prompt = f"""
    You are an expert evaluator for code-related information retrieval systems.

    User Query: {query}

    Below are samples from the retrieved context:
    {'---'.join(context_samples)}

    Please evaluate the retrieved context on a scale of 1-10 for the following metrics:
    """

    for metric in metrics:
        if metric == "relevance":
            evaluation_prompt += """
            - Relevance (1-10): How well does the retrieved information relate to the query?
              Score 1: Information is completely irrelevant
              Score 10: Information is highly relevant and directly answers the query
            """
        elif metric == "coverage":
            evaluation_prompt += """
            - Coverage (1-10): How comprehensive is the retrieved information?
              Score 1: Misses key information needed to address the query
              Score 10: Provides complete information needed to address the query
            """
        elif metric == "diversity":
            evaluation_prompt += """
            - Diversity (1-10): How diverse and non-redundant is the retrieved information?
              Score 1: Highly redundant information with many duplicates
              Score 10: Diverse set of relevant information with minimal redundancy
            """

    evaluation_prompt += """
    For each metric, provide only a numerical score (1-10) followed by a one-sentence justification.

    Then provide a brief overall analysis (2-3 sentences) of the retrieved context quality.
    """

    try:
        # Get evaluation from LLM
        evaluation_response = llm_client.invoke(evaluation_prompt)
        evaluation_text = evaluation_response.content if hasattr(evaluation_response, 'content') else str(
            evaluation_response)

        # Extract scores using regex
        import re
        print("\nMetrics:")
        for metric in metrics:
            pattern = rf"{metric.capitalize()}\s*\(?1-10\)?\s*:\s*(\d+)"
            match = re.search(pattern, evaluation_text, re.IGNORECASE)
            if match:
                score = int(match.group(1))
                print(f"- {metric.capitalize()}: {score}/10")
            else:
                print(f"- {metric.capitalize()}: 0/10 (score not found)")
                logging.warning(f"Could not extract score for {metric}")

        # Extract analysis
        analysis_pattern = r"overall analysis.*?:(.*?)(?=\n\n|\Z)"
        analysis_match = re.search(analysis_pattern, evaluation_text, re.IGNORECASE | re.DOTALL)
        if analysis_match:
            analysis = analysis_match.group(1).strip()
        else:
            # Fallback to extracting the last paragraph
            paragraphs = evaluation_text.split('\n\n')
            analysis = paragraphs[-1].strip()

        print(f"\nAnalysis: {analysis}")

    except Exception as e:
        logging.error(f"Error during retrieval evaluation: {e}")
        print(f"\nEvaluation failed: {str(e)}")


def evaluate_generated_answer(
        query: str,
        retrieved_context: dict[str, any],
        generated_answer: str,
        llm_client: any,
        ground_truth: str = None,
        metrics: list[str] = ["factuality", "completeness", "conciseness", "coherence"]
) -> None:
    """
    Evaluates the quality of the generated answer based on the query and retrieved context
    and prints the results.

    Args:
        query: The original user query
        retrieved_context: The context dictionary returned by retrieve_context()
        generated_answer: The answer generated by the RAG system
        llm_client: An LLM client for evaluation scoring
        ground_truth: Optional ground truth answer for comparison
        metrics: List of metrics to evaluate (factuality, completeness, conciseness, coherence)
    """
    logging.info("Evaluating generated answer quality...")
    print("\n===== Answer Evaluation =====")

    # Print basic info
    print(f"Query: {query}")
    print(f"Answer length: {len(generated_answer)} characters")
    if ground_truth:
        print(f"Ground truth available: Yes")
    else:
        print(f"Ground truth available: No")

    # Create context summary for the evaluation
    context_summary = []

    if retrieved_context.get("top_entities"):
        entity_names = [f"{e.get('metadata', {}).get('name', 'Unknown')}"
                        for e in retrieved_context.get("top_entities", [])[:5]]
        context_summary.append(f"Top entities: {', '.join(entity_names)}")

    if retrieved_context.get("relevant_chunks"):
        file_sources = set([c.get('metadata', {}).get('source_file', 'Unknown')
                            for c in retrieved_context.get("relevant_chunks", [])[:5]])
        context_summary.append(f"Code from files: {', '.join(file_sources)}")

    # Create evaluation prompt
    evaluation_prompt = f"""
    You are an expert evaluator for code-based question answering systems.

    User Query: {query}

    Retrieved Context Summary:
    {' | '.join(context_summary)}

    Generated Answer to Evaluate:
    {generated_answer}
    """

    if ground_truth:
        evaluation_prompt += f"""
        Ground Truth Answer:
        {ground_truth}
        """

    evaluation_prompt += """
    Please evaluate the generated answer on a scale of 1-10 for the following metrics:
    """

    for metric in metrics:
        if metric == "factuality":
            evaluation_prompt += """
            - Factuality (1-10): Does the answer contain only statements that are supported by the retrieved context?
              Score 1: Contains multiple unsupported or incorrect statements
              Score 10: All statements are accurate and supported by the context
            """
        elif metric == "completeness":
            evaluation_prompt += """
            - Completeness (1-10): Does the answer address all aspects of the query?
              Score 1: Ignores major aspects of the query
              Score 10: Comprehensively addresses all aspects of the query
            """
        elif metric == "conciseness":
            evaluation_prompt += """
            - Conciseness (1-10): Is the answer appropriately concise without missing important details?
              Score 1: Excessively verbose or too terse
              Score 10: Optimal length with all necessary information
            """
        elif metric == "coherence":
            evaluation_prompt += """
            - Coherence (1-10): Is the answer well-structured, logical, and easy to follow?
              Score 1: Disorganized, confusing, or difficult to follow
              Score 10: Logically structured, clear, and easy to understand
            """

    evaluation_prompt += """
    For each metric, provide only a numerical score (1-10) followed by a one-sentence justification.

    Then provide:
    1. A brief analysis (2-3 sentences) of the answer's overall quality
    2. Specific suggestions for improvement (2-3 points)
    """

    try:
        # Get evaluation from LLM
        evaluation_response = llm_client.invoke(evaluation_prompt)
        evaluation_text = evaluation_response.content if hasattr(evaluation_response, 'content') else str(
            evaluation_response)

        # Extract scores using regex
        import re
        print("\nMetrics:")
        for metric in metrics:
            pattern = rf"{metric.capitalize()}\s*\(?1-10\)?\s*:\s*(\d+)"
            match = re.search(pattern, evaluation_text, re.IGNORECASE)
            if match:
                score = int(match.group(1))
                print(f"- {metric.capitalize()}: {score}/10")
            else:
                print(f"- {metric.capitalize()}: 0/10 (score not found)")
                logging.warning(f"Could not extract score for {metric}")

        # Extract analysis
        analysis_pattern = r"(?:analysis|quality).*?:(.*?)(?=Specific suggestions|\d\.|improvement|$)"
        analysis_match = re.search(analysis_pattern, evaluation_text, re.IGNORECASE | re.DOTALL)
        if analysis_match:
            analysis = analysis_match.group(1).strip()
            print(f"\nAnalysis: {analysis}")

        # Extract improvement suggestions
        suggestions_pattern = r"(?:suggestions|improvement).*?:(.*?)(?=\Z)"
        suggestions_match = re.search(suggestions_pattern, evaluation_text, re.IGNORECASE | re.DOTALL)
        if suggestions_match:
            suggestions = suggestions_match.group(1).strip()
            print(f"\nImprovement Suggestions: {suggestions}")

    except Exception as e:
        logging.error(f"Error during answer evaluation: {e}")
        print(f"\nEvaluation failed: {str(e)}")

    print("\n" + "=" * 30)


def evaluate_rag_vs_llm(
        query: str,
        rag_answer: str,
        llm_only_answer: str,
        llm_client: any,
) -> None:
    """
    Compares a RAG-generated answer against an LLM-only answer using 3 specialized quality metrics.

    Args:
        query: The original user query
        rag_answer: The answer generated by the RAG system
        llm_only_answer: The answer generated by using only the LLM without retrieval
        llm_client: An LLM client for evaluation scoring
    """
    logging.info("Evaluating RAG vs LLM-only performance...")
    print("\n===== RAG vs LLM-Only Evaluation =====")

    # Print basic info
    print(f"Query: {query}")
    print(f"RAG answer length: {len(rag_answer)} characters")
    print(f"LLM-only answer length: {len(llm_only_answer)} characters")

    # Define the specialized quality metrics
    metrics = [
        "factual_correctness",
        "context_relevance",
        "hallucination_avoidance"
    ]

    # Create evaluation prompt
    evaluation_prompt = f"""
     You are an expert evaluator for comparing RAG (Retrieval-Augmented Generation) systems against pure LLM approaches.

     User Query: {query}

     RAG System Answer:
     {rag_answer}

     LLM-Only System Answer:
     {llm_only_answer}

     Please compare these answers using the following 3 specialized quality metrics:

     1. Factual Correctness (1-10): How factually accurate is each answer regarding technical details, code concepts, and implementation specifics?
        - Score 1: Contains many factual errors or misleading information
        - Score 5: Contains some minor inaccuracies but is generally reliable
        - Score 10: Provides completely accurate technical information with precise details

     2. Context Relevance (1-10): How well does the answer incorporate relevant code-specific context from the codebase or technical ecosystem?
        - Score 1: Generic response with no specific contextual references
        - Score 5: Some contextual references but missing key domain-specific information
        - Score 10: Rich with domain-specific details, file references, and ecosystem knowledge

     3. Hallucination Avoidance (1-10): How well does the answer avoid generating fabricated information?
        - Score 1: Makes up specific details like function names, file paths, or features that aren't confirmed
        - Score 5: Provides some speculative information but clearly marks it as such
        - Score 10: Only states what can be verified; when uncertain, explicitly acknowledges limits

     For each metric, provide:
     1. A score for the RAG system (1-10)
     2. A score for the LLM-only system (1-10)
     3. A 1-2 sentence justification for each score
     4. Which system performed better (RAG, LLM-Only, or Tie)

     Then provide:
     1. An overall winner (RAG, LLM-Only, or Tie) with the average score for each system
     2. A brief analysis (2-3 sentences) explaining when RAG provides clear advantages
     3. A brief analysis (2-3 sentences) explaining when the LLM-only approach might be sufficient
     """

    try:
        # Get evaluation from LLM
        evaluation_response = llm_client.invoke(evaluation_prompt)
        evaluation_text = evaluation_response.content if hasattr(evaluation_response, 'content') else str(
            evaluation_response)

        # Extract scores using regex
        import re
        print("\nQuality Metrics:")

        overall_rag_score = 0
        overall_llm_score = 0

        for metric in metrics:
            # Pattern to extract RAG score for this metric
            rag_pattern = rf"{metric.replace('_', ' ')}.*?RAG.*?(\d+)"
            rag_match = re.search(rag_pattern, evaluation_text, re.IGNORECASE | re.DOTALL)

            # Pattern to extract LLM-only score for this metric
            llm_pattern = rf"{metric.replace('_', ' ')}.*?LLM.*?(\d+)"
            llm_match = re.search(llm_pattern, evaluation_text, re.IGNORECASE | re.DOTALL)

            if rag_match and llm_match:
                rag_score = int(rag_match.group(1))
                llm_score = int(llm_match.group(1))

                overall_rag_score += rag_score
                overall_llm_score += llm_score

                # Determine winner for this metric
                winner = "RAG" if rag_score > llm_score else "LLM-Only" if llm_score > rag_score else "Tie"

                formatted_metric = metric.replace('_', ' ').title()
                print(f"- {formatted_metric}: RAG: {rag_score}/10, LLM-Only: {llm_score}/10, Winner: {winner}")
            else:
                formatted_metric = metric.replace('_', ' ').title()
                print(f"- {formatted_metric}: Scores not found")
                logging.warning(f"Could not extract scores for {metric}")

        # Calculate average scores
        if metrics:
            avg_rag_score = overall_rag_score / len(metrics)
            avg_llm_score = overall_llm_score / len(metrics)
            overall_winner = "RAG" if avg_rag_score > avg_llm_score else "LLM-Only" if avg_llm_score > avg_rag_score else "Tie"

            print(f"\nOverall Scores:")
            print(f"- RAG System: {avg_rag_score:.1f}/10")
            print(f"- LLM-Only System: {avg_llm_score:.1f}/10")
            print(f"- Overall Winner: {overall_winner}")

        # Extract RAG advantages analysis
        rag_advantages_pattern = r"(?:RAG provides|RAG advantages|when RAG).*?:(.*?)(?=LLM-only|when the LLM|$)"
        rag_advantages_match = re.search(rag_advantages_pattern, evaluation_text, re.IGNORECASE | re.DOTALL)

        if rag_advantages_match:
            rag_advantages = rag_advantages_match.group(1).strip()
            print(f"\nWhen RAG provides advantages: {rag_advantages}")

        # Extract LLM-only sufficiency analysis
        llm_sufficient_pattern = r"(?:LLM-only approach|when the LLM|LLM might be).*?:(.*?)(?=\Z)"
        llm_sufficient_match = re.search(llm_sufficient_pattern, evaluation_text, re.IGNORECASE | re.DOTALL)

        if llm_sufficient_match:
            llm_sufficient = llm_sufficient_match.group(1).strip()
            print(f"\nWhen LLM-only approach is sufficient: {llm_sufficient}")

    except Exception as e:
        logging.error(f"Error during RAG vs LLM-only evaluation: {e}")
        print(f"\nEvaluation failed: {str(e)}")

    print("\n" + "=" * 30)



def generate_llm_only_answer(query: str, llm_client: any) -> str:
    """
    Generates an answer using only the LLM without any retrieved context.

    Args:
        query: The user query
        llm_client: The LLM client to use for generation

    Returns:
        The generated answer from the LLM only
    """
    prompt = f"""
    Answer the following code-related question to the best of your ability:

    {query}

    Provide a detailed, accurate and helpful response.
    """

    response = llm_client.invoke(prompt)
    return response.content if hasattr(response, 'content') else str(response)
